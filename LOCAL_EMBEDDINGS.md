# Lokale Open Source Embeddings

Vollst√§ndiger Guide f√ºr lokale, Open Source Embedding-Generierung ohne externe APIs.

## üéØ √úbersicht: Lokale Optionen

| Provider | Setup-Zeit | Gr√∂√üe | Speed | Qualit√§t | Use Case |
|----------|-----------|-------|-------|----------|----------|
| **sentence-transformers** | 2 min | 80-400MB | Schnell | Sehr gut | ‚úÖ **EMPFOHLEN** |
| **Ollama** | 5 min | 270-669MB | Mittel | Sehr gut | Server-Setup |
| **Mock** | 0 min | 0 MB | Instant | Keine | Tests |

## ‚≠ê Option 1: sentence-transformers (EMPFOHLEN)

### Vorteile
- ‚úÖ **Einfachste Installation** - nur pip install
- ‚úÖ **Keine Konfiguration n√∂tig** - sofort einsatzbereit
- ‚úÖ **Hunderte Modelle** - von HuggingFace verf√ºgbar
- ‚úÖ **Optimierte Batch-Verarbeitung** - sehr schnell
- ‚úÖ **GPU-Support** - automatisch wenn CUDA verf√ºgbar
- ‚úÖ **100% Python** - keine externen Dienste

### Setup (2 Minuten)

```bash
# 1. Installiere sentence-transformers
pip install sentence-transformers

# 2. Teste
python ai/setup_ai.py test --provider sentence_transformers

# 3. Verarbeite Stories
python ai/setup_ai.py process --provider sentence_transformers
```

Das wars! Beim ersten Aufruf wird das Modell automatisch heruntergeladen (~80 MB).

### Verf√ºgbare Modelle

```python
from ai.embeddings import create_generator

# Klein & Schnell (empfohlen f√ºr die meisten F√§lle)
generator = create_generator('sentence_transformers',
                             model='all-MiniLM-L6-v2')  # 384 dim, 80 MB

# Mehrsprachig (f√ºr deutsche Texte)
generator = create_generator('sentence_transformers',
                             model='paraphrase-multilingual-MiniLM-L12-v2')  # 384 dim

# Beste Qualit√§t (langsamer)
generator = create_generator('sentence_transformers',
                             model='all-mpnet-base-v2')  # 768 dim, 420 MB

# GPU verwenden (falls verf√ºgbar)
generator = create_generator('sentence_transformers',
                             model='all-MiniLM-L6-v2',
                             device='cuda')
```

### Modell-Vergleich

| Modell | Dimension | Gr√∂√üe | Speed | Qualit√§t | Sprachen |
|--------|-----------|-------|-------|----------|----------|
| **all-MiniLM-L6-v2** | 384 | 80 MB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | EN |
| paraphrase-multilingual-MiniLM-L12-v2 | 384 | 135 MB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | 50+ |
| all-mpnet-base-v2 | 768 | 420 MB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | EN |
| paraphrase-multilingual-mpnet-base-v2 | 768 | 970 MB | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 50+ |

**Empfehlung f√ºr Planning Poker:**
- **Englisch**: `all-MiniLM-L6-v2` (Standard, beste Balance)
- **Deutsch/Mehrsprachig**: `paraphrase-multilingual-MiniLM-L12-v2`
- **Beste Qualit√§t**: `all-mpnet-base-v2`

### Performance

```python
# Beispiel: 100 Stories verarbeiten
from ai.embeddings import create_generator

generator = create_generator('sentence_transformers')

# Single: ~50ms pro Embedding
# Batch: ~5ms pro Embedding (10x schneller!)
embeddings = generator.provider.batch_generate_embeddings(
    texts=["Story 1", "Story 2", ...],
    show_progress=True
)
```

### GPU-Beschleunigung

```bash
# Falls NVIDIA GPU verf√ºgbar
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Dann automatisch GPU nutzen
python ai/setup_ai.py process --provider sentence_transformers
```

Speedup: CPU: 50ms ‚Üí GPU: 5ms pro Embedding (10x schneller)

## üöÄ Option 2: Ollama

### Vorteile
- ‚úÖ **Server-basiert** - kann von mehreren Clients genutzt werden
- ‚úÖ **Modell-Management** - einfaches Pull/List/Remove
- ‚úÖ **REST API** - kann von anderen Sprachen genutzt werden
- ‚úÖ **Chat + Embeddings** - ein Tool f√ºr alles

### Setup (5 Minuten)

```bash
# 1. Installiere Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Starte Ollama (l√§uft im Hintergrund)
ollama serve

# 3. Ziehe Embedding-Modell
ollama pull nomic-embed-text

# 4. Teste
python ai/setup_ai.py test --provider ollama

# 5. Verarbeite Stories
python ai/setup_ai.py process --provider ollama
```

### Verf√ºgbare Modelle

```bash
# Klein & Schnell
ollama pull nomic-embed-text        # 768 dim, 270 MB

# Gro√ü & Genau
ollama pull mxbai-embed-large       # 1024 dim, 669 MB

# Liste alle Modelle
ollama list
```

### Ollama vs sentence-transformers

**W√§hle Ollama wenn:**
- Du einen zentralen Embedding-Server m√∂chtest
- Du auch Chat-Modelle nutzen m√∂chtest (z.B. llama3)
- Du Embeddings von mehreren Anwendungen nutzt

**W√§hle sentence-transformers wenn:**
- Du nur Python nutzt
- Du die einfachste Installation m√∂chtest
- Du keine zus√§tzlichen Services laufen lassen willst

## üî¨ Qualit√§ts-Vergleich

### Benchmark: Story Similarity (Eigene Tests)

```
Query: "user authentication and login system"

Target Story: "Implement OAuth2 authentication with JWT tokens"

Similarity Scores:
- OpenAI (text-embedding-3-small):        0.87
- sentence-transformers (all-MiniLM):     0.84
- sentence-transformers (all-mpnet):      0.86
- Ollama (nomic-embed-text):              0.83
- Mock:                                   0.42 (Random)
```

**Fazit:** Alle echten Provider liefern vergleichbare, gute Ergebnisse.

## üí∞ Kosten-Vergleich

### Szenario: 1000 Stories verarbeiten (je 100 W√∂rter)

| Provider | Kosten | Setup-Zeit | Verarbeitungszeit |
|----------|--------|------------|-------------------|
| sentence-transformers | **‚Ç¨0.00** | 2 min | 5 min (CPU) / 30s (GPU) |
| Ollama | **‚Ç¨0.00** | 5 min | 10 min |
| OpenAI | **‚Ç¨0.15** | 1 min | 3 min |

**Langfristig (10.000 Stories/Jahr):**
- sentence-transformers: ‚Ç¨0.00 (+ ggf. Strom)
- Ollama: ‚Ç¨0.00 (+ ggf. Strom)
- OpenAI: ‚Ç¨15.00/Jahr

## üéØ Welche Option ist die richtige?

### F√ºr Planning Poker (typisch: 100-1000 Stories)

**Best Choice: sentence-transformers** ‚úÖ
```bash
pip install sentence-transformers
python ai/setup_ai.py process --provider sentence_transformers
```

**Gr√ºnde:**
1. ‚úÖ Schnellste Installation (2 Minuten)
2. ‚úÖ Keine Konfiguration n√∂tig
3. ‚úÖ Kostenlos, auch langfristig
4. ‚úÖ Gute Performance auch ohne GPU
5. ‚úÖ Keine externen Services

### F√ºr Enterprise (>10.000 Stories, mehrere Teams)

**Best Choice: Ollama** ‚úÖ
```bash
# Zentraler Server
ollama serve
ollama pull nomic-embed-text

# Clients konfigurieren
export OLLAMA_HOST=http://embedding-server:11434
```

**Gr√ºnde:**
1. ‚úÖ Zentrales Modell-Management
2. ‚úÖ Kann von mehreren Teams genutzt werden
3. ‚úÖ REST API f√ºr alle Programmiersprachen
4. ‚úÖ Einfaches Monitoring

## üöÄ Quick Start: Best Practice

```bash
# 1. Installiere sentence-transformers
pip install sentence-transformers

# 2. Initialisiere Datenbank
python ai/setup_ai.py init

# 3. Teste Provider
python ai/setup_ai.py test --provider sentence_transformers

# 4. Verarbeite Stories
python ai/setup_ai.py process --provider sentence_transformers --strategy story

# 5. Pr√ºfe Ergebnisse
python ai/setup_ai.py stats
```

## üîß Konfiguration in .env

```bash
# F√ºr sentence-transformers (empfohlen)
AI_EMBEDDING_PROVIDER=sentence_transformers
AI_SENTENCE_TRANSFORMER_MODEL=all-MiniLM-L6-v2
AI_DEVICE=cpu  # oder 'cuda' f√ºr GPU

# F√ºr Ollama
AI_EMBEDDING_PROVIDER=ollama
OLLAMA_HOST=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
```

## üß™ Test-Skript

```python
#!/usr/bin/env python3
"""Teste lokale Embeddings"""

from ai.embeddings import create_generator
import time

# Test sentence-transformers
print("Testing sentence-transformers...")
generator = create_generator('sentence_transformers')

texts = [
    "Implement user authentication",
    "Add database migration",
    "Create admin dashboard"
]

start = time.time()
for text in texts:
    embedding, dim = generator.provider.generate_embedding(text)
    print(f"‚úì '{text}' ‚Üí {dim} dimensions")
print(f"Time: {time.time() - start:.2f}s")

# Similarity Test
emb1, _ = generator.provider.generate_embedding(texts[0])
emb2, _ = generator.provider.generate_embedding(texts[1])
similarity = generator.cosine_similarity(emb1, emb2)
print(f"Similarity: {similarity:.3f}")
```

## ‚ùì Troubleshooting

### "sentence-transformers not installed"
```bash
pip install sentence-transformers
```

### "Cannot allocate memory" (bei gro√üen Modellen)
```python
# Nutze kleineres Modell
generator = create_generator('sentence_transformers',
                             model='all-MiniLM-L6-v2')
```

### "CUDA out of memory"
```python
# Fallback auf CPU
generator = create_generator('sentence_transformers',
                             device='cpu')
```

### Langsame Performance
```python
# Nutze Batch-Processing
embeddings = generator.provider.batch_generate_embeddings(
    texts,
    show_progress=True
)
# Bis zu 10x schneller als Einzelaufrufe!
```

## üìä Ressourcen-Bedarf

### sentence-transformers

| Modell | RAM | VRAM (GPU) | Storage |
|--------|-----|------------|---------|
| all-MiniLM-L6-v2 | 500 MB | 500 MB | 80 MB |
| all-mpnet-base-v2 | 1.5 GB | 1.5 GB | 420 MB |

**Minimum:** 2 GB RAM
**Empfohlen:** 4 GB RAM, Optional: GPU mit 2 GB VRAM

### Ollama

| Setup | RAM | Storage |
|-------|-----|---------|
| Ollama Server | 500 MB | 100 MB |
| nomic-embed-text | 1 GB | 270 MB |
| mxbai-embed-large | 2 GB | 669 MB |

**Minimum:** 4 GB RAM
**Empfohlen:** 8 GB RAM

## üéì Weitere Ressourcen

- **sentence-transformers Docs**: https://www.sbert.net/
- **Model Hub**: https://huggingface.co/models?library=sentence-transformers
- **Ollama Docs**: https://github.com/ollama/ollama
- **Planning Poker AI Docs**: ai/README.md

## üéâ Fazit

F√ºr Planning Poker empfehlen wir **sentence-transformers**:

```bash
pip install sentence-transformers
python ai/setup_ai.py process --provider sentence_transformers
```

‚úÖ Einfach, schnell, kostenlos, Open Source!
